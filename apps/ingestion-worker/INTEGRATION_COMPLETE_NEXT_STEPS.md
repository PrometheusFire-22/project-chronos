# âœ… Modal GPU Integration - Ready for Testing

## What We Just Fixed

### ğŸ¯ THE BLOCKER - SOLVED!

**Before (BROKEN âŒ):**
```python
# ingestion_service.py line 22
self.converter = DocumentConverter()  # Ran Docling locally â†’ OOM errors!
```

**After (FIXED âœ…):**
```python
# ingestion_service.py line 24
self.modal_fn = modal.Function.lookup("chronos-docling", "process_document")
# Calls Modal GPU remotely â†’ No OOM, 10-20x faster!
```

---

## ğŸ“Š Complete Changes

### 1. Refactored `ingestion_service.py`
- âŒ Removed: Local Docling (causes OOM)
- âœ… Added: Modal GPU remote calls
- âœ… Added: Better logging with emojis for progress tracking
- âœ… Added: `markdown_content` storage in database
- âœ… Added: Processing time and cost tracking

### 2. Updated `models.py`
- âœ… Added: `markdown_content TEXT` column to `DocumentRaw`

### 3. Created Quality Assessment Tools
- âœ… `scripts/quality_assessment.py` - All 3 assessment methods
- âœ… Visual markdown comparison
- âœ… JSON structure inspection
- âœ… RAG query testing

### 4. Modal Secrets Configured
- âœ… `chronos-openai` â†’ OPENAI_API_KEY
- âœ… `chronos-database` â†’ DATABASE_URL
- âœ… `chronos-directus` â†’ DIRECTUS_URL, DIRECTUS_ACCESS_TOKEN

### 5. Documentation
- âœ… `ARCHITECTURE_SUMMARY.md` - Complete reference
- âœ… Backup strategy explained (R2 versioning + lifecycle policies)
- âœ… Cost breakdowns and storage recommendations

---

## ğŸš€ Next Steps to Get Pipeline Working

### Step 1: Run Database Migration

The `markdown_content` column needs to be added to the database.

**Option A: Manual SQL (Quick)**
```bash
# Connect to PostgreSQL
psql $DATABASE_URL

# Create ingestion schema if it doesn't exist
CREATE SCHEMA IF NOT EXISTS ingestion;

# Run migration from alembic/versions/4108aca2dd8b (if not done yet)
# Then add markdown column
ALTER TABLE ingestion.documents_raw
ADD COLUMN IF NOT EXISTS markdown_content TEXT;

COMMENT ON COLUMN ingestion.documents_raw.markdown_content IS
'Full markdown with emojis âœ…ğŸš€ğŸ“Š for debugging and rechunking';

# Create full-text search index
CREATE INDEX idx_documents_raw_markdown_fts
ON ingestion.documents_raw
USING gin(to_tsvector('english', markdown_content));
```

**Option B: Via Alembic (Proper)**
```bash
cd /home/prometheus/coding/finance/project-chronos

# Set database URL (from .env or deployment/lightsail/.env)
export DATABASE_URL="<your-database-url>"

# Run all pending migrations
poetry run alembic upgrade head
```

### Step 2: Update Modal Function with Secrets

The Modal GPU function needs access to secrets for potential future use:

```bash
cd /home/prometheus/coding/finance/project-chronos

# Redeploy Modal function with secrets
poetry run modal deploy apps/ingestion-worker/modal_functions/docling_processor.py
```

**Update `docling_processor.py` to use secrets:**
```python
@app.function(
    gpu="A10G",
    timeout=600,
    memory=8192,
    retries=2,
    secrets=[
        modal.Secret.from_name("chronos-openai"),      # For future use
        modal.Secret.from_name("chronos-database"),    # For direct DB writes
        modal.Secret.from_name("chronos-directus"),    # For file access
    ]
)
def process_document(pdf_bytes: bytes, file_id: str, source_url: str = None):
    ...
```

### Step 3: Test Modal Function (Isolated)

Verify Modal GPU processing works:

```bash
cd /home/prometheus/coding/finance/project-chronos/apps/ingestion-worker

# Test with your service list PDF
poetry run modal run modal_functions/docling_processor.py \
  --test-pdf-path "test-service-list.pdf"

# Expected output:
# âœ… Success!
#    Processing time: 27.03s
#    Page count: 10
#    Markdown length: 24,471 chars
#    GPU cost: ~$0.0083
```

### Step 4: Start FastAPI Service Locally

```bash
cd /home/prometheus/coding/finance/project-chronos/apps/ingestion-worker

# Set environment variables (from .env or deployment/lightsail/.env)
export DATABASE_URL="<your-database-url>"
export OPENAI_API_KEY="<your-openai-api-key>"
export DIRECTUS_URL="https://admin.automatonicai.com"
export DIRECTUS_ACCESS_TOKEN="<your-directus-token>"

# Start service
poetry run uvicorn main:app --reload --port 8000

# Service should start and connect to Modal:
# âœ… Modal GPU function connected: chronos-docling.process_document
```

### Step 5: Test Full Pipeline End-to-End

#### Option A: Manual Trigger (Recommended for first test)

```bash
# Upload a PDF to Directus manually via web UI
# Then trigger processing via API:

curl -X POST http://localhost:8000/ingest \
  -H "Content-Type: application/json" \
  -d '{
    "file_id": "<directus-file-id>",
    "metadata": {"test": true}
  }'

# Watch logs for:
# ğŸ“„ Processing document: filename.pdf
# ğŸš€ Sending to Modal GPU for processing...
# âœ… Modal GPU processing complete:
#    - Processing time: 27.03s
#    - Pages: 10
# ğŸ’¾ Saving to PostgreSQL...
# ğŸ‰ Document processing complete!
```

#### Option B: Directus Webhook (Full integration)

```bash
# 1. Configure Directus webhook:
#    - URL: http://localhost:8000/webhook/directus (or AWS URL)
#    - Event: files.create
#    - Collection: directus_files

# 2. Upload PDF to Directus
# 3. Watch FastAPI logs for automatic processing
```

### Step 6: Validate Results

```bash
# Check database for processed document
psql $DATABASE_URL -c "
  SELECT
    id,
    file_name,
    LENGTH(markdown_content) as markdown_length,
    (docling_data->>'page_count') as pages,
    created_at
  FROM ingestion.documents_raw
  ORDER BY created_at DESC
  LIMIT 1;
"

# Check chunks were created
psql $DATABASE_URL -c "
  SELECT COUNT(*), AVG(LENGTH(text_content))
  FROM ingestion.document_chunks
  WHERE document_id = '<doc-id-from-above>';
"
```

### Step 7: Quality Assessment

```bash
cd /home/prometheus/coding/finance/project-chronos/apps/ingestion-worker

# Option 1: Visual markdown comparison
poetry run python scripts/quality_assessment.py visual \
  --file-id <doc-uuid> \
  --output comparison.md

# Option 2: JSON structure inspection
poetry run python scripts/quality_assessment.py inspect \
  --file-id <doc-uuid>

# Option 3: RAG query testing
poetry run python scripts/quality_assessment.py query \
  --file-id <doc-uuid> \
  --question "What services are listed in section 3?"
```

---

## ğŸ¯ After Testing: Next Priorities

Once the pipeline works end-to-end, move to:

### 1. **CHRONOS-502: Webhook Authentication** (Security Critical)
```python
# Add to main.py
from fastapi import Header, HTTPException

WEBHOOK_SECRET = os.getenv("WEBHOOK_SECRET")

@app.post("/webhook/directus")
async def directus_webhook(
    body: DirectusWebhookPayload,
    x_directus_signature: str = Header(None)
):
    # Validate signature
    if not verify_signature(body, x_directus_signature, WEBHOOK_SECRET):
        raise HTTPException(status_code=401, detail="Invalid signature")
    ...
```

### 2. **Deploy to AWS Lightsail** (CHRONOS-505)
```bash
# SSH to AWS
ssh chronos@16.52.210.100

# Clone/pull code
cd ~/project-chronos
git pull origin feat/CHRONOS-496-modal-gpu-integration

# Install dependencies
cd apps/ingestion-worker
poetry install

# Set environment variables in .env
# Start with systemd or Docker
```

### 3. **Build Graph Extraction Pipeline** (Your Main Goal)

Once RAG is working, layer on Apache AGE for knowledge graph:

```python
# Example: Extract entities from chunks
from llama_index.llms import OpenAI

llm = OpenAI(model="gpt-4")

prompt = f"""
Extract financial entities and relationships from this text:

{chunk.text_content}

Return JSON with:
{{
  "entities": [
    {{"type": "GP", "name": "..."}},
    {{"type": "Fund", "name": "..."}},
    {{"type": "Asset", "name": "..."}}
  ],
  "relationships": [
    {{"source": "GP1", "relation": "manages", "target": "Fund1"}},
    {{"source": "Fund1", "relation": "holds", "target": "Asset1"}}
  ]
}}
"""

result = llm.complete(prompt)
entities = json.loads(result.text)

# Store in Apache AGE graph database
conn.execute("""
  SELECT * FROM cypher('financial_graph', $$
    CREATE (gp:GP {name: $gp_name})
    CREATE (fund:Fund {name: $fund_name})
    CREATE (gp)-[:MANAGES]->(fund)
  $$) as (result agtype);
""", {"gp_name": "...", "fund_name": "..."})
```

---

## ğŸ“š Context: Your GTHA RE Investment Intelligence Platform

**Goal**: Map out GTHA real estate investment networks by extracting entities and relationships from financial/legal docs.

**Entity Types:**
- GPs (General Partners)
- LPs (Limited Partners)
- Funds (RE, Private Credit, Fund Finance)
- SPVs (Special Purpose Vehicles)
- Assets (Properties, Loans)

**Relationship Types:**
- `manages` (GP â†’ Fund)
- `invested_in` (LP â†’ Fund)
- `lends_to` (Credit Fund â†’ Asset/Fund)
- `holds` (SPV â†’ Asset)
- `is_investment_advisor_to` (GP â†’ Fund)

**Tech Stack:**
- âœ… Docling + Modal GPU â†’ Extract text/structure from PDFs
- âœ… LlamaIndex + pgvector â†’ RAG for document Q&A
- ğŸ”„ GPT-4 + Prompt Engineering â†’ Extract entities/relationships
- ğŸ”„ Apache AGE (graph DB) â†’ Store network data
- ğŸ”„ NetworkX â†’ Analyze network topology
- ğŸ”„ GUI (React + Next.js) â†’ Visualize and query graph

**Use Cases:**
- **Liquidity Events**: Find who's connected to distressed assets
- **Deal Sourcing**: Map co-investment networks
- **Due Diligence**: Trace capital flows and manager relationships
- **Market Intelligence**: Identify key players in GTHA RE

---

## ğŸ‰ Summary

**What's Working:**
- âœ… Modal GPU function (tested, 27s processing, $0.0083/doc)
- âœ… Refactored ingestion service (no more local Docling!)
- âœ… Quality assessment tools (3 methods)
- âœ… Modal secrets configured
- âœ… Documentation complete

**What's Needed:**
- ğŸ”§ Run database migration (5 minutes)
- ğŸ”§ Test full pipeline end-to-end (10 minutes)
- ğŸ”§ Add webhook authentication (30 minutes)
- ğŸ”§ Deploy to AWS Lightsail (20 minutes)

**Then Build:**
- ğŸš€ Entity/relationship extraction (LLM prompting)
- ğŸš€ Apache AGE graph database
- ğŸš€ Network analysis (NetworkX)
- ğŸš€ Visualization GUI

**You're 80% done with the RAG foundation. Time to layer on the graph extraction!** ğŸ“ŠğŸ”—ğŸ¯
